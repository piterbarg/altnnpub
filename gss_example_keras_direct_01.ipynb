{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and test the generalized Stochastic Sampling (gSS) model \n",
    "\n",
    "## In this notebook do it \"by hand\", ie using granular interfaces such as the ``Keras`` functional interface\n",
    "\n",
    "Here we create a ``hidim`` version of the model with the ``Adam`` optimizer for the frequency bounds (aka scales) and linear regression for the outer (linear) weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import cm\n",
    "from tensorflow import keras\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.keras import TqdmCallback\n",
    "import tensorflow.keras.backend as K\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "# our stuff\n",
    "from nnu import points_generator as pgen\n",
    "from nnu import laplace_kernel, fit_function_factory\n",
    "from nnu import gss_kernels, gss_layer\n",
    "\n",
    "# globals\n",
    "np.set_printoptions(precision =3, suppress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use floatXX\n",
    "keras_dtype = 'float32'\n",
    "tf.keras.backend.set_floatx(keras_dtype)\n",
    "tf_dtype = tf.float32 if keras_dtype == 'float32' else tf.float64\n",
    "np_dtype = np.float32 if keras_dtype == 'float32' else np.float64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the function we want to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 2\n",
    "laplace_mixture = fit_function_factory.KernelType.LpM\n",
    "stds = [1.5, 1.0, 0.5][-ndim:]\n",
    "off_diag_correl = 0.0\n",
    "laplace_shift = 1\n",
    "means = np.array([[1, 1, 0], [-1, -1, 0], [0.5, -0.5, 0]])\n",
    "means = means[:, :ndim]\n",
    "cov_multipliers = [0.5, 0.3, 0.1]  \n",
    "mix_weights = [0.4, 0.35, 0.35]  \n",
    "covar_matr = laplace_kernel.simple_covar_matrix(stds, off_diag_correl)\n",
    "\n",
    "function_to_fit = fit_function_factory.generate_nd(\n",
    "    laplace_mixture, covar_matr, \n",
    "    shift=laplace_shift,\n",
    "    means=means,\n",
    "    cov_multipliers=cov_multipliers,\n",
    "    mix_weights=mix_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the learning set (inputX, inputY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nsamples = 10000\n",
    "input_seed = 1917\n",
    "sim_range = 4\n",
    "\n",
    "points_type = \"random\"\n",
    "inputX = pgen.generate_points(sim_range, nsamples, ndim, points_type, seed = input_seed)[0]\n",
    "inputY = function_to_fit(inputX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate nodes for our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nodes_type = \"random\"\n",
    "# nodes_type=\"regular\"\n",
    "nodes_seed = 2022\n",
    "\n",
    "nnodes = 200\n",
    "nsr_stretch = 1.2\n",
    "nodes_sim_range = sim_range * nsr_stretch \n",
    "\n",
    "nodes = pgen.generate_points(\n",
    "    nodes_sim_range, nnodes, ndim, nodes_type, seed = nodes_seed, plot = 0)[0]\n",
    "nnodes = len(nodes)\n",
    "\n",
    "nnodes_per_dim = round(pow(nnodes, 1./ndim))\n",
    "global_scale = pgen.average_distance(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a generalized stochastic sampling (gSS) model\n",
    "We use Keras's functional interface, where we perform regression on the outer coefs for each guess of the inner ones\n",
    "Note that here we are setting up a ``hidim`` (see paper) version of the model, but this could be changed by setting ``scales_dim`` to other values of ``gss_layer.ScalesDim`` enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_knots=False\n",
    "optimize_scales=True\n",
    "scales_dim=gss_layer.ScalesDim.OnePerKnot # this is for hidim flavour of the model\n",
    "apply_final_aggr = False\n",
    "\n",
    "scales_init = np.ones((1,ndim)) if scales_dim != gss_layer.ScalesDim.OnePerKnot else np.ones_like(nodes)\n",
    "\n",
    "print_model_summary = True\n",
    "\n",
    "fake_dim = 1\n",
    "k_fake_input = keras.Input((fake_dim,), name = 'input', dtype=tf_dtype)\n",
    "\n",
    "# \"sideload\" inputX -- a node that always returns inputX\n",
    "xvals = keras.layers.Lambda(\n",
    "    lambda _: tf.constant(inputX),\n",
    "    input_dim = fake_dim,\n",
    "    name='xpts'\n",
    ")(k_fake_input)\n",
    "\n",
    "# \"sideload\" inputY -- a node that always returns inputY\n",
    "yvals = keras.layers.Lambda(\n",
    "    lambda _: tf.expand_dims(tf.constant(inputY),-1),\n",
    "    input_dim = fake_dim,\n",
    "    name='ypts'\n",
    ")(k_fake_input)\n",
    "\n",
    "# Construct  ProdKernelLayer for inputX. Here we have some trainable parameters that will later be optimized\n",
    "# most typically scales. Nodes have been pre-set\n",
    "prod_kernel = gss_kernels.lanczos_kernel(a=2.0, freq_bound=0.9/global_scale, tf_dtype = tf_dtype)\n",
    "per_coord_kernels_at_input_x = gss_layer.ProdKernelLayer(\n",
    "    input_dim = ndim,\n",
    "    knots=nodes,\n",
    "    scales=scales_init,\n",
    "    optimize_knots=optimize_knots,\n",
    "    optimize_scales=optimize_scales,\n",
    "    scales_dim=scales_dim,\n",
    "    activation = prod_kernel,\n",
    "    name = 'prodkernel'\n",
    ")(xvals)\n",
    "\n",
    "# Apply coordinatewise product to the output of ProdKernelLayer to get actual kernels for inputX\n",
    "kernels_at_input_x = keras.layers.Lambda(\n",
    "    lambda x: K.prod(x, axis=2),\n",
    "    name='product'\n",
    ")(per_coord_kernels_at_input_x)\n",
    "\n",
    "# Regress inputY on the product kernels evaluated at inputX\n",
    "regr_xy = keras.layers.Lambda(\n",
    "    lambda xy: tf.linalg.lstsq(xy[0],xy[1],l2_regularizer=0.01),\n",
    "    name = 'regr_xy'\n",
    ")([kernels_at_input_x,yvals])\n",
    "\n",
    "regr_model = keras.Model(\n",
    "    inputs=k_fake_input, outputs=regr_xy, name=\"regr_xy_model\")\n",
    "regr_model.build(input_shape=(fake_dim,))\n",
    "if print_model_summary:\n",
    "    regr_model.summary()\n",
    "\n",
    "regr_model_output = regr_model(k_fake_input)\n",
    "\n",
    "# Now predict the values of y from the regression (in the optimizer, for fixed values of scales)\n",
    "predict_y  = keras.layers.Lambda(\n",
    "    lambda xc: tf.matmul(xc[0],xc[1]),\n",
    "    name = 'predict_y'\n",
    ")([kernels_at_input_x, regr_model_output])\n",
    "\n",
    "# first build a model that predicts y so we can examine the results later\n",
    "predict_model = keras.Model(inputs=k_fake_input, outputs=predict_y, name=\"predict_y_model\")\n",
    "predict_model.build(input_shape = (fake_dim,))\n",
    "\n",
    "if print_model_summary:\n",
    "    predict_model.summary()\n",
    "\n",
    "predict_model_output = predict_model(k_fake_input)\n",
    "\n",
    "# now add the residual\n",
    "resid_xy  = keras.layers.Subtract(\n",
    "    name = 'resid_xy'\n",
    ")([yvals,predict_model_output])\n",
    "\n",
    "# optionally sum up the squares inside the model \n",
    "if apply_final_aggr:\n",
    "    resid_xy = keras.layers.Lambda(\n",
    "        lambda z:tf.reduce_sum(tf.square(z)),\n",
    "        name = 'sumsq' \n",
    "    )(resid_xy)\n",
    "\n",
    "# and create a model for the residual -- this is the one to optimize against zero\n",
    "model = keras.Model(inputs=k_fake_input, outputs=resid_xy, name=\"fit_model\")\n",
    "\n",
    "model.build(input_shape = (fake_dim,))\n",
    "\n",
    "if print_model_summary:\n",
    "    model.summary()\n",
    "\n",
    "# record starting point\n",
    "init_weights = model.get_weights().copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up and run the Adam optimizer for the frequency bounds (aka inner weights/scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1 # x100\n",
    "batch_size = nsamples\n",
    "learn_rate = 0.1\n",
    "use_tb_callback = False\n",
    "use_tqdm = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_weights(init_weights)\n",
    "\n",
    "output_dim = 1 if apply_final_aggr else batch_size\n",
    "fake_x = np.zeros((output_dim, fake_dim), dtype=np_dtype)\n",
    "fake_y = np.zeros(output_dim, dtype=np_dtype)\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=learn_rate)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=opt, metrics=['mse','mae'])\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "callbacks = [TqdmCallback(verbose=0)] if use_tqdm else []\n",
    "if use_tb_callback:\n",
    "    callbacks.append(tensorboard_callback)\n",
    "\n",
    "start_time = timer()\n",
    "stats_before = model.evaluate(fake_x, fake_y, batch_size= batch_size)\n",
    "model.fit(fake_x, fake_y, epochs=int(100*n_epochs), batch_size=batch_size, \n",
    "    verbose=0, callbacks=callbacks)\n",
    "stats_after = model.evaluate(fake_x, fake_y, batch_size= batch_size)\n",
    "fit = predict_model.predict(fake_x, batch_size= batch_size)\n",
    "end_time = timer()\n",
    "print(f'Time ellapsed = {end_time - start_time:.1f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib auto\n",
    "%matplotlib inline\n",
    "print(f'stats before fit = {(stats_before)}') \n",
    "print(f'stats after fit =  {(stats_after)}')\n",
    "learn_error=np.linalg.norm(fit[:,0] - inputY)/np.linalg.norm(inputY)\n",
    "print(f'r2 = {1 - learn_error:.4f}')\n",
    "plt.plot(fit[:,0], inputY, '.')\n",
    "plt.title('learn: actual vs fit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib auto\n",
    "%matplotlib inline\n",
    "plot_step = 1\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.scatter(inputX[::plot_step,0], inputX[::plot_step,1],  inputY[::plot_step],\n",
    "            cmap=cm.coolwarm, marker='.', alpha = 0.75, s=1, label = 'actual')\n",
    "ax.scatter(inputX[::plot_step,0], inputX[::plot_step,1],  fit[::plot_step,0],\n",
    "            cmap=cm.coolwarm, marker='.', alpha = 0.75, s = 1, label = 'fit')\n",
    "\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model suitable for prediction on xs other than inputX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = keras.Sequential()\n",
    "\n",
    "# our main layer\n",
    "test_model.add(gss_layer.ProdKernelLayer(\n",
    "    input_dim=ndim,\n",
    "    knots=nodes,\n",
    "    scales=scales_init,\n",
    "    optimize_knots=optimize_knots,\n",
    "    optimize_scales=optimize_scales,\n",
    "    scales_dim=scales_dim,\n",
    "    activation=prod_kernel,\n",
    "    name='prodkernel'\n",
    "))\n",
    "\n",
    "# coordniate-wise product\n",
    "test_model.add(keras.layers.Lambda(\n",
    "    lambda x: K.prod(x, axis=2),\n",
    "    name='product'\n",
    "))\n",
    "\n",
    "# final aggregation\n",
    "test_model.add(keras.layers.Dense(\n",
    "    1,\n",
    "    activation='linear',\n",
    "    name='final',\n",
    "    use_bias=False,\n",
    "))\n",
    "\n",
    "test_model.build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_shape = model.layers[-1].output_shape\n",
    "if output_shape == ():\n",
    "    output_size = 1\n",
    "else:\n",
    "    output_size = output_shape[0]\n",
    "\n",
    "\n",
    "predict_model = model.get_layer('predict_y_model')\n",
    "regr_model = predict_model.get_layer('regr_xy_model')\n",
    "inner_weights = regr_model.get_layer('prodkernel').get_weights()\n",
    "outer_weights = regr_model.predict(fake_x, batch_size=output_size)\n",
    "\n",
    "\n",
    "test_model.get_layer('prodkernel').set_weights(inner_weights)\n",
    "test_model.get_layer('final').set_weights([outer_weights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check that we recover the same outputs for model and test_model when using the same inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = test_model.predict(inputX)\n",
    "plt.plot(fit[:, 0], test_y, '.')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare prediction vs actual for an an independent set of points (test set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "testX = pgen.generate_points(\n",
    "    sim_range, nsamples, ndim, points_type, seed=123)[0]\n",
    "testY = function_to_fit(testX)\n",
    "\n",
    "mc_error = 1-np.linalg.norm(inputY)/np.linalg.norm(testY)\n",
    "\n",
    "testFit = test_model.predict(testX)\n",
    "\n",
    "test_error = np.linalg.norm(testFit[:, 0] - testY)/np.linalg.norm(testY)\n",
    "print(f'testing set r2 = {1 - test_error:.4f}')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.plot(testFit[:, 0], testY, '.')\n",
    "plt.title('test: actual vs fit')\n",
    "plt.show()\n",
    "\n",
    "print(f\"mc error  :{mc_error:.4f}\")\n",
    "print(f\"test error: {test_error:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c75fd95ee6a8673db96eeaeea4fa8e27c3cb071aef74affedb213b23408cb297"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
