{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and test the generalized Stochastic Sampling (gSS) model\n",
    "## This notebook uses ``gss_model_factory`` and other higher-level interfaces \n",
    "\n",
    "We create a ``onedim`` version of the model with a one-dim optimizer for the frequency bounds (aka scales) and linear regression for the outer (linear) weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "from matplotlib import cm\n",
    "from tensorflow import keras\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# our stuff\n",
    "from nnu import points_generator as pgen\n",
    "from nnu import laplace_kernel, fit_function_factory\n",
    "from nnu import gss_layer, gss_model_factory, gss_report_config\n",
    "\n",
    "# globals\n",
    "np.set_printoptions(precision =3, suppress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the function we want to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim=2\n",
    "laplace_mixture = fit_function_factory.KernelType.LpM\n",
    "stds = [1.5, 1.0, 0.5][-ndim:]\n",
    "off_diag_correl = 0.0\n",
    "laplace_shift = 1\n",
    "means = np.array([[1, 1, 0], [-1, -1, 0], [0.5, -0.5, 0]])\n",
    "means = means[:, :ndim]\n",
    "cov_multipliers = [0.5, 0.3, 0.1]  \n",
    "mix_weights = [0.4, 0.35, 0.35]  \n",
    "covar_matr = laplace_kernel.simple_covar_matrix(stds, off_diag_correl)\n",
    "\n",
    "function_to_fit = fit_function_factory.generate_nd(\n",
    "    laplace_mixture, covar_matr, \n",
    "    shift=laplace_shift,\n",
    "    means=means,\n",
    "    cov_multipliers=cov_multipliers,\n",
    "    mix_weights=mix_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the learning set (inputX, inputY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nsamples = 10000\n",
    "input_seed = 1917\n",
    "sim_range = 4\n",
    "\n",
    "points_type = \"random\"\n",
    "inputX = pgen.generate_points(sim_range, nsamples, ndim, points_type, seed = input_seed)[0]\n",
    "inputY = function_to_fit(inputX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate nodes for our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nodes_type = \"random\"\n",
    "# nodes_type=\"regular\"\n",
    "nodes_seed = 2022\n",
    "\n",
    "nnodes = 200\n",
    "nsr_stretch = 1.2\n",
    "nodes_sim_range = sim_range * nsr_stretch \n",
    "\n",
    "nodes = pgen.generate_points(\n",
    "    nodes_sim_range, nnodes, ndim, nodes_type, seed = nodes_seed, plot = 0)[0]\n",
    "nnodes = len(nodes)\n",
    "\n",
    "nnodes_per_dim = round(pow(nnodes, 1./ndim))\n",
    "global_scale = pgen.average_distance(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the gSS ``onedim`` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dtype=tf.float32 # tf.float64\n",
    "np_dtype = np.float32 if tf_dtype == tf.float32 else np.float64\n",
    "\n",
    "# common specs we will be re-using a few times\n",
    "model_specs = dict(\n",
    "    model_type=gss_report_config.ModelType.SSL,\n",
    "    use_outer_regression=True,\n",
    "    optimize_knots=False,\n",
    "    optimize_scales=True,\n",
    "    scales_dim=gss_layer.ScalesDim.OnlyOne,\n",
    "    apply_final_aggr=False,\n",
    "    kernel='invquad',\n",
    ")\n",
    "\n",
    "seed_for_keras = 2021\n",
    "model = gss_model_factory.generate_model(\n",
    "    ndim=ndim,\n",
    "    global_scale=global_scale,\n",
    "    nodes=nodes,\n",
    "    inputX=inputX,\n",
    "    inputY=inputY,\n",
    "    scales=None,\n",
    "    seed_for_keras=seed_for_keras,\n",
    "\n",
    "    **model_specs,\n",
    ")\n",
    "\n",
    "# Because of how the model is constructed, fit(...) uses some fake xs. \n",
    "# The real inputX, inputY is 'side-loaded' into the model through 'xpts' and 'ypts' layers\n",
    "\n",
    "fake_dim = 1\n",
    "output_size =  nsamples\n",
    "x_to_use = np.zeros((output_size, fake_dim), dtype=np_dtype)\n",
    "y_to_use = np.zeros(output_size, dtype=np_dtype)\n",
    "\n",
    "model.summary()\n",
    "model.get_layer('predict_y_model').summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculatre average gradients\n",
    "We create a 'test' model to calculate gradients. The test model\n",
    "will have its outer weights (linear coefficients) calculated by regression\n",
    "so this is the \"base\" approximation from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grad_model = gss_model_factory.generate_model_for_testing_2(\n",
    "    model,\n",
    "    ndim=ndim,\n",
    "    global_scale=global_scale,\n",
    "    nodes=nodes,\n",
    "\n",
    "    **model_specs,\n",
    "\n",
    "    average_slopes=None,\n",
    "    l2_regularizer=1e-8,\n",
    "    tf_dtype=tf.float32,\n",
    "    nsr_stretch=nsr_stretch,\n",
    "    generate_more_nodes=False, \n",
    "    sim_range=sim_range)\n",
    "\n",
    "# calc the gradients using tf\n",
    "x_tensor = tf.convert_to_tensor(inputX, dtype=tf_dtype)\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x_tensor)\n",
    "    output = grad_model(x_tensor)\n",
    "    gradients = t.gradient(output, x_tensor).numpy()\n",
    "\n",
    "# the average directyional risk magnitudes\n",
    "average_slopes = np.linalg.norm(gradients, axis=0)\n",
    "average_slopes /= np.amax(average_slopes)\n",
    "print(\"Normalized directional risk magnitudes: \", average_slopes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-init the model with calculated average slopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print_params = True\n",
    "# Redefine the model now that we know average_slopes\n",
    "model = gss_model_factory.generate_model(\n",
    "    ndim=ndim,\n",
    "    global_scale=global_scale,\n",
    "    nodes=nodes,\n",
    "    inputX=inputX,\n",
    "    inputY=inputY,\n",
    "    scales=None,\n",
    "    seed_for_keras=seed_for_keras,\n",
    "\n",
    "    **model_specs,\n",
    "\n",
    "    average_slopes=average_slopes,\n",
    "    sim_range=sim_range,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the gSS model\n",
    "We run a one-dim optimizer for the global scale of the kernels with linear regression for outer (linear) coefficients on each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get access to the inner weight corresponding to the common scale (known to be a scalar in this particular case)\n",
    "#  that drives all scales so we can set it directly and optimize in a 1d optimizer\n",
    "predict_model = model.get_layer('predict_y_model')\n",
    "prod_kernel = predict_model.get_layer('prodkernel')\n",
    "orig_weights = prod_kernel.get_weights()\n",
    "\n",
    "def obj_f_1d(w):\n",
    "    '''\n",
    "    1D objective function for scipy.optimize.minimize_scalar\n",
    "    '''\n",
    "    # set the right weight to w, reuse the others (nodes -- not trainable here)\n",
    "    prod_kernel.set_weights([np.array([[w]]), orig_weights[1]])\n",
    "\n",
    "    fit = predict_model.predict(x_to_use, batch_size=nsamples)\n",
    "    mse = np.linalg.norm(fit[:, 0] - inputY)/np.linalg.norm(inputY)\n",
    "    return mse\n",
    "\n",
    "res = scipy.optimize.minimize_scalar(\n",
    "    obj_f_1d, bounds=(0.25, 1.25), method='bounded', options={'xatol': 1e-1})\n",
    "\n",
    "# This sets the scale to the final solution. Prints it for good measure\n",
    "achieved_mse = obj_f_1d(res.x)\n",
    "print(\n",
    "    f'1d optimizer found solution scale = {res.x:.4f} and achieved mse = {achieved_mse:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the results of the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_model = model.get_layer('predict_y_model')\n",
    "\n",
    "fit = predict_model.predict(x_to_use, batch_size=nsamples)\n",
    "learn_mse = np.linalg.norm(fit[:, 0] - inputY)/np.linalg.norm(inputY)\n",
    "learn_mae = np.linalg.norm(\n",
    "    fit[:, 0] - inputY, ord=1)/np.linalg.norm(inputY)/np.sqrt(nsamples)  # note we divide by L2 norm on purpose\n",
    "\n",
    "print(f'learn_mse = {learn_mse:.4f}, learn_mae = {learn_mae:.4f}')\n",
    "\n",
    "# %matplotlib auto\n",
    "%matplotlib inline\n",
    "plt.plot(fit[:,0], inputY, '.')\n",
    "plt.title('learn: actual vs fit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the fit in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib auto\n",
    "# %matplotlib inline\n",
    "plot_step = 1\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.scatter(inputX[::plot_step,0], inputX[::plot_step,1],  inputY[::plot_step],\n",
    "            cmap=cm.coolwarm, marker='.', alpha = 0.75, s=1, label = 'actual')\n",
    "ax.scatter(inputX[::plot_step,0], inputX[::plot_step,1],  fit[::plot_step,0],\n",
    "            cmap=cm.coolwarm, marker='.', alpha = 0.75, s = 1, label = 'fit')\n",
    "\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the final model with an increased number of nodes using calibrated scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = gss_model_factory.generate_model_for_testing_2(\n",
    "    model,\n",
    "    ndim=ndim,\n",
    "    global_scale=global_scale,\n",
    "    nodes=nodes,\n",
    "\n",
    "    **model_specs,\n",
    "\n",
    "    average_slopes=average_slopes,\n",
    "    l2_regularizer=1e-8,\n",
    "    tf_dtype=tf.float32,\n",
    "    nsr_stretch=nsr_stretch,\n",
    "    generate_more_nodes=True, \n",
    "    sim_range=sim_range)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See the results of the fit for an independently generates test set (testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res_seed=314\n",
    "\n",
    "testX = pgen.generate_points(sim_range, nsamples, ndim, \n",
    "    points_type, seed = test_res_seed)[0]\n",
    "testY = function_to_fit(testX)\n",
    "\n",
    "test_fit = test_model.predict(testX)\n",
    "\n",
    "test_mse = np.linalg.norm(test_fit[:, 0] - testY)/np.linalg.norm(testY)\n",
    "test_mae = np.linalg.norm(\n",
    "    test_fit[:, 0] - testY, ord=1)/np.linalg.norm(testY)/np.sqrt(nsamples)  # note we divide by L2 norm on purpose\n",
    "\n",
    "\n",
    "print(f'test_mse = {test_mse:.4f}, test_mae = {test_mae:.4f}')\n",
    "\n",
    "# %matplotlib auto\n",
    "%matplotlib inline\n",
    "plt.plot(test_fit[:,0], testY, '.')\n",
    "plt.title('Test: actual vs fit')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib auto\n",
    "# %matplotlib inline\n",
    "plot_step = 1\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.scatter(testX[::plot_step,0], testX[::plot_step,1],  testY[::plot_step],\n",
    "            cmap=cm.coolwarm, marker='.', alpha = 0.75, s=1, label = 'actual')\n",
    "ax.scatter(testX[::plot_step,0], testX[::plot_step,1],  test_fit[::plot_step,0],\n",
    "            cmap=cm.coolwarm, marker='.', alpha = 0.75, s = 1, label = 'fit')\n",
    "\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot outer weights as a function of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = test_model.get_layer('final').get_weights()[0][:, 0]\n",
    "test_nodes=test_model.get_layer('prodkernel').get_weights()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib auto\n",
    "# %matplotlib inline\n",
    "\n",
    "plot_step = 1\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.scatter(test_nodes[::plot_step,0], test_nodes[::plot_step,1], ws[::plot_step],\n",
    "            c = ws[::plot_step],\n",
    "            cmap=cm.coolwarm, marker='.',  alpha = 0.75, label = 'outer weights')\n",
    "\n",
    "plt.xlabel('nodes_x')\n",
    "plt.ylabel('nodex_y')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c75fd95ee6a8673db96eeaeea4fa8e27c3cb071aef74affedb213b23408cb297"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
